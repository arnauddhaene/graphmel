{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8d8626-8cbb-4f69-acef-87d167778c03",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e341d1-dba0-4364-936e-40baef49188e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa7193c3-6d2b-4ec7-80b6-b20767602772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Tuple, List, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8567fac6-471c-4b20-ab17-ecdb489ffb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6675b9e5-f801-4fb7-82f3-0775581ab41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import ConcatDataset, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.transforms import ToDense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "628063e2-8cdf-4507-9af6-70cfd3081ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "90f11227-6c54-47c4-957d-fffe0da195f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../src/'))\n",
    "\n",
    "from src.utils import FILES, DATA_FOLDERS, extract_study_phase, fetch_data, Preprocessor\n",
    "\n",
    "CONNECTION_DIR = '/Users/arnauddhaene/Downloads/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1be44e-f3ab-4f7c-a6fa-b8b528bf3cef",
   "metadata": {},
   "source": [
    "### Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6e332d5-aec2-4266-8073-4d1a086fd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test = load_dataset(dense=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c623056-6ea7-49b6-a07b-cc106bf96f29",
   "metadata": {},
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63e55364-96a6-4557-9be6-c5c7c6cf03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07f342bc-5f1b-483a-bf70-c2028b133b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold no. 0\n",
      "Train size: 56, Valid size: 15\n",
      "Intersection: 0\n",
      "Fold no. 1\n",
      "Train size: 57, Valid size: 14\n",
      "Intersection: 0\n",
      "Fold no. 2\n",
      "Train size: 57, Valid size: 14\n",
      "Intersection: 0\n",
      "Fold no. 3\n",
      "Train size: 57, Valid size: 14\n",
      "Intersection: 0\n",
      "Fold no. 4\n",
      "Train size: 57, Valid size: 14\n",
      "Intersection: 0\n"
     ]
    }
   ],
   "source": [
    "for fold, (I_train, I_valid) in enumerate(kfold.split(dataset_train)):\n",
    "    \n",
    "    print(f'Fold no. {fold}')\n",
    "    print(f'Train size: {len(I_train)}, Valid size: {len(I_valid)}')\n",
    "    print(f'Intersection: {len(list(set(I_train) & set(I_valid)))}')\n",
    "    \n",
    "    sampler_train = SubsetRandomSampler(I_train)\n",
    "    sampler_valid = SubsetRandomSampler(I_valid)\n",
    "    \n",
    "    loader_train = DenseDataLoader(dataset_train, batch_size=8, sampler=sampler_train)\n",
    "    loader_valid = DenseDataLoader(dataset_train, batch_size=8, sampler=sampler_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b0f6a-609c-49f6-87b4-f139d494fbba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Rewrites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f757b-35a2-45eb-ae1f-d53e550da076",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Rewritten `preprocess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "276c1b39-9ba2-41e4-9b7f-0546e6372839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    labels: pd.Series, lesions: pd.DataFrame, patients: pd.DataFrame,\n",
    "    test_size: float = 0.2, seed: int = 27, verbose: int = 1\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Preprocess filtered raw data into train, validation, and test splits.\n",
    "    Imputation, standardization, and one-hot encoding of features using sklearn pipelines.\n",
    "\n",
    "    Args:\n",
    "        labels (pd.Series): `gpcr_id` indexed Series with progression labels. 1 is NPD.\n",
    "        lesions (pd.DataFrame): gpcr_id` indexed lesion-level data.\n",
    "        patients (pd.DataFrame): `gpcr_id` indexed patient-level data including blood screens.\n",
    "        test_size (float, optional): Ratio of test set. Defaults to 0.2.\n",
    "        seed (int, optional): Random seed. Defaults to 27.\n",
    "        verbose (int, optional): tuneable parameter for output verbosity. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "            * X_train (pd.DataFrame): `gpcr_id` indexed training dataset.\n",
    "            * X_test (pd.DataFrame): `gpcr_id` indexed testing dataset.\n",
    "            * y_train (pd.Series): `gpcr_id` indexed training Series with progression labels. 1 is NPD.\n",
    "            * y_test (pd.Series): `gpcr_id` indexed test Series with progression labels. 1 is NPD.\n",
    "    \"\"\"\n",
    "    \n",
    "    I_train, I_test, y_train, y_test = \\\n",
    "        train_test_split(labels.index, labels, test_size=test_size, random_state=seed)\n",
    "        \n",
    "    lesions_pp = Preprocessor(\n",
    "        pipe=ColumnTransformer(\n",
    "            [('scaler', StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "             ('one-hot', OneHotEncoder(),\n",
    "              make_column_selector(dtype_include=object))]),\n",
    "        feats_out_fn=lambda c: c.transformers_[0][-1] + list(c.transformers_[1][1].categories_[0])\n",
    "    )\n",
    "    \n",
    "    patients_numerical = list(patients.select_dtypes(np.number).columns)\n",
    "    patients_categorical = list(patients.select_dtypes([bool, object]).columns)\n",
    "    patients_categorical.remove('immuno_therapy_type')\n",
    "\n",
    "    features_range = list(range(len(patients_numerical) + len(patients_categorical) + 1))\n",
    "    bp = np.cumsum([len(patients_numerical), len(patients_categorical), 1])\n",
    "\n",
    "    clf_patients = Pipeline([\n",
    "        ('imputers', ColumnTransformer([\n",
    "            ('median', SimpleImputer(strategy='median'), patients_numerical),\n",
    "            ('frequent', SimpleImputer(strategy='most_frequent'), patients_categorical)\n",
    "        ], remainder='passthrough')),\n",
    "        ('preprocess', ColumnTransformer([\n",
    "            ('scaler', StandardScaler(), features_range[0:bp[0]]),\n",
    "            ('one-hot', OneHotEncoder(handle_unknown='ignore'),\n",
    "             features_range[bp[0]:bp[1]]),\n",
    "            ('count-vec', CountVectorizer(analyzer=set), features_range[bp[1]:bp[2]][0])\n",
    "        ], remainder='passthrough')),\n",
    "    ])\n",
    "\n",
    "    patients_pp = Preprocessor(\n",
    "        pipe=clf_patients,\n",
    "        feats_out_fn=lambda c: (c.named_steps['imputers'].transformers_[0][2] \\\n",
    "                                + list(c.named_steps['preprocess'].transformers_[1][1].get_feature_names()) \\\n",
    "                                + c.named_steps['preprocess'].transformers_[2][1].get_feature_names())\n",
    "    )\n",
    "\n",
    "    lesions_pp.fit(lesions.loc[I_train])\n",
    "\n",
    "    lesions_train = lesions_pp.transform(lesions.loc[I_train])\n",
    "    lesions_test = lesions_pp.transform(lesions.loc[I_test])\n",
    "    \n",
    "    patients_pp.fit(patients.loc[I_train])\n",
    "\n",
    "    patients_train = patients_pp.transform(patients.loc[I_train])\n",
    "    patients_test = patients_pp.transform(patients.loc[I_test])\n",
    "    \n",
    "    X_train = pd.merge(lesions_train, patients_train, left_index=True, right_index=True)\n",
    "    X_test = pd.merge(lesions_test, patients_test, left_index=True, right_index=True)\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print('Processed and split dataset into \\n' \\\n",
    "              + f'  Train: {y_train.shape[0]} \\n' \\\n",
    "              + f'  Test: {y_test.shape[0]}')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a3e81-0b18-455e-829f-dcf2cc341abc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Rewritten `load_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02c781df-fad7-4437-a00b-ab3395edd967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(\n",
    "    connectivity: str = 'wasserstein', test_size: float = 0.2, seed: int = 27,\n",
    "    dense: bool = False, verbose: int = 0\n",
    ") -> Tuple[List[Data], List[Data]]:\n",
    "    \"\"\"\n",
    "    Get training, validation, and testing DataLoaders.\n",
    "    Mainly used as a high-level data fetcher in the running script.\n",
    "\n",
    "    Args:\n",
    "        connectivity (str, optional): node connectivity method.. Defaults to 'wasserstein'.\n",
    "        batch_size (int, optional): [description]. Defaults to 8.\n",
    "        test_size (float, optional): Ratio of test set. Defaults to 0.2.\n",
    "        seed (int, optional): Random seed. Defaults to 27.\n",
    "        dense (bool, optional): Output a DenseDataLoader\n",
    "        verbose (int, optional): tuneable parameter for output verbosity. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Data], List[Data]]:\n",
    "            * loader_train (List[Data]): packaged training dataset.\n",
    "            * loader_test (List[Data]): packaged testing dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    labels, lesions, patients = fetch_data(verbose)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        preprocess(labels, lesions, patients,\n",
    "                   test_size=test_size, seed=seed,\n",
    "                   verbose=verbose)\n",
    "        \n",
    "    dataset_train = create_dataset(X=X_train, Y=y_train, dense=dense,\n",
    "                                  connectivity=connectivity, verbose=verbose)\n",
    "    \n",
    "    # In the test loader we set the batch size to be\n",
    "    # equal to the size of the whole test set\n",
    "    dataset_test = create_dataset(X=X_test, Y=y_test, dense=dense,\n",
    "                                 connectivity=connectivity, verbose=verbose)\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print('Final amount of datapoints \\n' \\\n",
    "              + f'  Train: {len(dataset_train)} \\n' \\\n",
    "              + f'  Test: {len(dataset_test)}')\n",
    "\n",
    "    return dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4f992-7f10-4745-9911-7679f16c3ac4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Rewritten `create_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4531fe13-97bd-4b1f-8abd-c461e67f76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "    X: pd.DataFrame, Y: pd.Series, dense: bool = False, connectivity: str = 'wasserstein',\n",
    "    distance: float = 0.5, verbose: int = 0\n",
    ") -> List[Data]:\n",
    "    \"\"\"Packages preprocessed data and its labels into a dataset\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): `gpcr_id` indexed datapoints (lesions)\n",
    "        Y (pd.Series): `gpcr_id` indexed labels. 1 is NPD.\n",
    "        dense (bool, optional): create dense Graph representations\n",
    "        connectivity (str, optional): node connectivity method. Defaults to 'wasserstein'.\n",
    "        distance (float, optional): if `wasserstein` connectivity is chosen,\n",
    "            the threshold distance in order to create an edge between nodes. Defaults to 0.5.\n",
    "        verbose (int, optional): tuneable parameter for output verbosity.. Defaults to 0.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: acceptable values for connectivity are: 'fully', 'organ', and 'wasserstein'\n",
    "\n",
    "    Returns:\n",
    "        dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    lesions = pd.read_csv(os.path.join(CONNECTION_DIR + DATA_FOLDERS[2], FILES[DATA_FOLDERS[2]]['lesions']))\n",
    "    # Filter out benign lesions and non-post-1 studies\n",
    "    lesions = lesions[(lesions.pars_classification_petct != 'benign') & (lesions.study_name == 'post-01')]\n",
    "    \n",
    "    dataset = []\n",
    "    skipped = 0\n",
    "    \n",
    "    if dense:\n",
    "        max_num_nodes = lesions.groupby('gpcr_id').size().max()\n",
    "        to_dense = ToDense(max_num_nodes)\n",
    "\n",
    "    for patient in list(X.index.unique()):\n",
    "\n",
    "        # Create patient sub-DataFrame of all his post-1 study lesions\n",
    "        pdf = lesions[lesions.gpcr_id == patient].reset_index()\n",
    "        \n",
    "        # Sanity check\n",
    "        assert pdf.shape[0] == X[X.index == patient].shape[0], f'Unequal lesion count for patient {patient}'\n",
    "        \n",
    "        num_nodes = pdf.shape[0]\n",
    "        edge_index = []\n",
    "        \n",
    "        # Skip single-noded graphs\n",
    "        if num_nodes < 2:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # Connect lesions using different methodologies\n",
    "        if connectivity == 'organ':\n",
    "            # Connect all lesions that are assigned to the same organ\n",
    "            for i in range(num_nodes):\n",
    "                source = pdf.loc[i].assigned_organ\n",
    "                targets = list(pdf[pdf.assigned_organ == source].index)\n",
    "\n",
    "                edge_index.extend([[i, j] for j in targets if i != j])\n",
    "                \n",
    "        elif connectivity == 'fully':\n",
    "            # Create a fully-connected network representation\n",
    "            edge_index = list(permutations(range(num_nodes), 2, ))\n",
    "            \n",
    "        elif connectivity == 'wasserstein':\n",
    "            # Use the mean and std of the SUV of each lesion to simulate SUV distributions (normal)\n",
    "            # and subsequently connect nodes with similar SUV distributions using the Wasserstein distance\n",
    "            # as a distance metric\n",
    "            for i in range(num_nodes):\n",
    "                source_mean, source_sd = pdf.loc[i].mean_suv_val, pdf.loc[i].sd_suv_val\n",
    "                source_distribution = np.random.normal(source_mean, source_sd, 1000)\n",
    "                \n",
    "                targets = [id for id, mu, sd in zip(pdf.index, pdf.mean_suv_val, pdf.sd_suv_val)\n",
    "                           if wasserstein_distance(source_distribution,\n",
    "                                                   np.random.normal(mu, sd, 1000)) < distance]\n",
    "\n",
    "                edge_index.extend([[i, j] for j in targets])\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f'Connectivity value not accepted: {connectivity}.'\n",
    "                             \"Must be either 'fully', 'wasserstein', or 'organ'.\")\n",
    "\n",
    "        edge_index = torch.tensor(edge_index).t().long()\n",
    "    \n",
    "        x = torch.tensor(X.loc[patient].reset_index(drop=True).to_numpy().astype(np.float32))\n",
    "        y = torch.tensor(Y.loc[patient])\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, num_nodes=num_nodes, y=y.reshape(-1))\n",
    "\n",
    "        dataset.append(to_dense(data) if dense else data)\n",
    "        \n",
    "    if verbose > 0 and skipped > 0:\n",
    "        print(f'Skipped {skipped} graphs as they have less than 2 nodes.')\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98340c-49bb-4f0a-b7f9-edadc0c89413",
   "metadata": {},
   "source": [
    "### TrainingMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0a8858f4-19a5-47cf-9e64-5a66243ee50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMetrics():\n",
    "\n",
    "    def __init__(self, run: int = 0):\n",
    "        \n",
    "        self.run = run\n",
    "        self.storage = []\n",
    "    \n",
    "    def log_metric(self, metric: str, value: float, step: int = 0):\n",
    "        \n",
    "        self.storage.append(dict(metric=metric, value=value, step=step, run=self.run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0ef3ba13-71c4-495c-a201-ed87be41e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = TrainingMetrics(run=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b822bb82-401a-4c1f-8a14-7903b236c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.log_metric('Accuracy - training', 0.94, 0)\n",
    "metrics.log_metric('Accuracy - training', 0.92, 1)\n",
    "metrics.log_metric('Accuracy - training', 0.93, 2)\n",
    "metrics.log_metric('Accuracy - validation', 0.81, 0)\n",
    "metrics.log_metric('Accuracy - validation', 0.51, 1)\n",
    "metrics.log_metric('Accuracy - validation', 0.91, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8fb26a8c-af83-4603-a62b-6ac0788a6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = TrainingMetrics(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "600b28a4-504c-42db-8672-0cb1617faf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.log_metric('Accuracy - training', 0.74, 0)\n",
    "n.log_metric('Accuracy - training', 0.72, 1)\n",
    "n.log_metric('Accuracy - training', 0.73, 2)\n",
    "n.log_metric('Accuracy - validation', 0.91, 0)\n",
    "n.log_metric('Accuracy - validation', 0.91, 1)\n",
    "n.log_metric('Accuracy - validation', 0.21, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "471eb95c-cd84-42d5-9051-5df27e4f6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    pd.DataFrame(metrics.storage), pd.DataFrame(n.storage)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e3b2e353-6b75-4911-ae06-9634ebdf874d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  metric    Accuracy - training\n",
       "  step                        0\n",
       "  value                0.141421\n",
       "  Name: 0, dtype: object),\n",
       " (1,\n",
       "  metric    Accuracy - training\n",
       "  step                        1\n",
       "  value                0.141421\n",
       "  Name: 1, dtype: object),\n",
       " (2,\n",
       "  metric    Accuracy - training\n",
       "  step                        2\n",
       "  value                0.141421\n",
       "  Name: 2, dtype: object),\n",
       " (3,\n",
       "  metric    Accuracy - validation\n",
       "  step                          0\n",
       "  value                  0.070711\n",
       "  Name: 3, dtype: object),\n",
       " (4,\n",
       "  metric    Accuracy - validation\n",
       "  step                          1\n",
       "  value                  0.282843\n",
       "  Name: 4, dtype: object),\n",
       " (5,\n",
       "  metric    Accuracy - validation\n",
       "  step                          2\n",
       "  value                  0.494975\n",
       "  Name: 5, dtype: object)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.groupby(['metric', 'step']).value.std().reset_index().iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c53c78ad-7687-468e-b6c4-08e1e3db90d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - training: 0.14142135623730948 | 0\n",
      "Accuracy - training: 0.1414213562373096 | 1\n",
      "Accuracy - training: 0.1414213562373096 | 2\n",
      "Accuracy - validation: 0.0707106781186547 | 0\n",
      "Accuracy - validation: 0.28284271247461906 | 1\n",
      "Accuracy - validation: 0.49497474683058335 | 2\n"
     ]
    }
   ],
   "source": [
    "for index, feature in df.groupby(['metric', 'step']).value.std().reset_index().iterrows():\n",
    "    print(f'{feature.metric}: {feature.value} | {feature.step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "826aaf1e-e99e-4e89-8c27-8123990fb157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b1a8a995-7801-4ca1-a35e-85fdf55128ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphConv(5, 5).reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f7cca3e0-6feb-4117-815f-c44e0df6800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d3235-0367-4800-b075-dd69fbe44404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
